---
title: "OLS regression, Part 1"
subtitle: "A model of the mean"
author: "Austin Hart"
institute: "American University"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, rladies, rladies-fonts]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message=FALSE, warning=FALSE, eval = TRUE, echo = FALSE, 
  fig.align = 'center', dev = 'svg'
)
```

```{r results='hide'}
  library(tidyverse)
  load("DCPS testing.RData")
  bit = read_csv('bitdata.csv')
```

# Data

- Relies on examples from:
  - DCPS testing.RData
  - bitdata.csv
  
```{r samset, eval=FALSE,echo=TRUE}
# Load packages
  library(tidyverse)

# Set project directory
  setwd("/Users/hart/Documents/my600folder")
  
# Load data
  load("DCPS testing.RData")
  bit = read_csv('bitdata.csv')
```

---
# Linear regression

- A model of the mean of $Y$, conditional on $X$
  - Mean of Y: $E(Y) = \mu_Y$
  - ...conditional on X: $E(Y|X) = \alpha + \beta_1X$

- *Most* appropriate when you have:
  - continuous outcome variable (DV/Y)
  - continuous exposure variable (IV/X)

---
# Nuts and bolts

- Estimate $Y = \alpha + \beta_1X + \beta_2Z +...$

- Present parameters of the line
  - $\alpha$ Intercept = $E(Y|X=0)$
  - $\beta_1$ Slope = $E(\Delta Y | \Delta X)$
  Change in $E(Y)$ for unit-increase in X, net of $Z$
  
  
---
class: inverse, middle, right

# LINEAR RELATIONSHIPS
### and basics of a line


---
# Regression and the mean outcome

- Unconditional mean of outcome variable, $Y$
$$
E(Y) = \mu
$$

- Mean outcome, $Y$, as a linear function of $X$
$$
E(Y|X) = \alpha + \beta_1 X_i
$$

---
# Example 1: DCPStesting.Rdata

- Unconditional mean of outcome variable, $Y$
$$
E(Y) = \mu
$$


```{r ex1, echo=TRUE}
  # unconditional mean
    mean(dcps$ProfLang)

    lm(ProfLang ~ 1, data = dcps)
```

---
# Example 2: DCPStesting.Rdata

- Mean outcome, $Y$, as a linear function of $X$
$$
E(Y|X) = \alpha + \beta_1 X_i
$$

```{r ex2, echo=TRUE}
  # mean conditional on X
    lm(ProfLang ~ ProfMath, data = dcps)
```

---
# Example 3: DCPStesting.Rdata

From prior estimates:
$$
E(ProfLang|ProfMath) = 4.3 + 0.94*ProfMath_i
$$

  Given the data, a school's expected language proficiency increases by 0.94-points for every one-point increase in math proficiency.
  

```{r fit1, fig.width=4, fig.height=4, dpi=250}
# Scatter w/OLS fit (numeric X, numeric Y)
  # 1. store OLS estimates
    est = lm(ProfLang ~ ProfMath, data = dcps)
  # 2. plot  
    plot(ProfLang ~ ProfMath, data = dcps) # scatter
    abline(est, col = 'cornflowerblue') # add linear fit
```



---
class: inverse, middle, right

# OLS ESTIMATION
### finding the line of best fit


---
# Population Regression Function

Model $Y$ as a linear function of $X$:
$$
Y_i = \alpha + \beta_1 X_i + e_i
$$

- The line: $E(Y|X) = \alpha + \beta_1X_i$
  - $\alpha$ Intercept = $E(Y|X=0)$

  - $\beta_1$ Slope = $E(\Delta Y | \Delta X)$

- The error term $e_i$
  - vertical distance between point and line $Y_i - (\alpha + \beta_1 X_i)$

  - reflects probabilistic nature of estimation (noise)

  - absorbs systematic, unexplained variation in Y (potential bias)


---
# Line of BEST fit

> OLS REGRESSION: method of estimating linear regression model. 
>
> Chooses parameters that minimize the "sum of squared errors".

```{r fit2, fig.width=4, fig.height=4, dpi=250}
# Scatter w/OLS fit (numeric X, numeric Y)
  # 1. store OLS estimates
    est = lm(ProfLang ~ ProfMath, data = dcps)
  # 2. plot  
    plot(ProfLang ~ ProfMath, data = dcps) # scatter
    abline(est, col = 'blue', lty=3, lw = 2) # add linear fit
```


---
class: inverse, middle, right

# Non-linearity
### and the log-transformation

---
# Skew and the sensitive mean

- Mean sensitive to outliers
$$
E(Y) = \mu_Y= \sum\frac{y_i}{n}
$$
  - Extreme values inflate numerator
  - Pulls mean from median
  - Mean becomes poor/biased measure of the middle
  
- Regression is a model of the mean
  - Skew in $X$ and/or $Y$ causes problems
  - Use log-transformation to "normalize" variables

---
# The bitdata.csv file

- Source: Haftel and Thompson (2013) *Delayed Ratification*
- Units of observation: Bilateral Investment Treaty
- Total observations: 2,595 BITs
- Primary outcome: time (months) to ratification

---
# Notice a BIT of skew?

```{r seeskew, echo=TRUE, fig.width=3.5, fig.height=2.5, dpi=250}
# Check the mean vs median
  summary(bit$month_count)

# Visualize
  hist(bit$month_count, main ='skewed')

```

---
# What to do with positive skew

- Got positive skew? Use the natural logarithm
  - Use only with positive values
  - Got zeroes? Use $+1$ fix
  
```{r skewfix1, echo=TRUE,fig.width=3.5, fig.height=2.5, dpi=250}
# Check the mean vs median
  summary(log(bit$month_count + 1))

# Use the natural logarithm to normalize
  hist(log(bit$month_count + 1), main = 'Log-transformed', xlab = '')

```

---
# Skew and regression

> Regress months to ratification on relative wealth gap

```{r skewreg,fig.width=4, fig.height=4, dpi=250}

e1 = lm(month_count ~ exp(log_gapgdppc), data = bit)
plot(month_count ~ exp(log_gapgdppc), data = bit, xlab = 'Gap in GDPpc')
abline(e1, col = 'red', lw = 3)

```

---
# What to do with that skew?

> Adjust for positive skew in both variables

```{r skewreg2,fig.width=4, fig.height=4, dpi=250}

e1 = lm(log(1+month_count) ~ log_gapgdppc, data = bit)
plot(log(1+month_count) ~ log_gapgdppc, data = bit)
abline(e1, col = 'green', lw = 3)

```

---
# Consequences of log-transformation

> Using logged variables changes interpretation of $\beta_1$

- $Y,~X$: 
  - *Unit* increase in X, $\beta$ *unit* change in Y
   
- $log(Y),~X$:
  - One *unit* increase in X, $(exp(\beta)-1)*100$ **percent** change in Y
   
- $Y,~log(X)$:
  - One **percent** increase in X, $\beta/100$ *unit* change in Y
   
- $log(X),~log(Y)$: 
  - One **percent** increase in X, $\beta$ **percent** change in Y

---
# BIT of an example

```{r puns, echo=TRUE}
lm(log(month_count + 1) ~ log_gapgdppc, data = bit)
```

I find that a 1% increase in wealth gap between signatories reduces the expected ratification spell by 0.08%.

